# -*- coding: utf-8 -*-
"""Helinskiopus-hi-en.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m9wqLDnOeq_W3ppz_6jwYZ-Nnzs2DpQK
"""

!pip install datasets transformers[sentencepiece] sacrebleu

import os
import sys
import transformers
import tensorflow as tf
from datasets import load_dataset
from transformers import AutoTokenizer
from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
from transformers import AdamWeightDecay
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

model_checkpoint = "Helsinki-NLP/opus-mt-hi-en"

from datasets import load_dataset
dataset = load_dataset("iitb.csv")

dataset
dataset['train'][1]

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
tokenizer("Hello, this is a sentence!")

max_input_length = 128
max_target_length = 128

source_lang = "hi"
target_lang = "en"


def preprocess_function(examples):
    inputs = [ex[source_lang] for ex in examples["translation"]]
    targets = [ex[target_lang] for ex in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

preprocess_function(dataset["train"][:2])
tokenized_datasets = dataset.map(preprocess_function, batched=True)

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

batch_size = 16
learning_rate = 2e-5
weight_decay = 0.01
num_train_epochs = 10

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")

generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128)

print(tokenized_datasets)

train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    batch_size=batch_size,
    shuffle=True,
    collate_fn=data_collator,
)
validation_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    batch_size=batch_size,
    shuffle=False,
    collate_fn=data_collator,
)
generation_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    batch_size=8,
    shuffle=False,
    collate_fn=generation_data_collator,
)

optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)
model.compile(optimizer=optimizer)
model.fit(train_dataset, validation_data=validation_dataset, epochs=5)

print(dataset['train'][0])

import sacrebleu
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

# Ensure the source and target language codes
source_lang = "en"
target_lang = "hi"

def compute_bleu_score(model, tokenizer, dataset, batch_size=16):
    predictions = []
    references = []

    for batch in dataset.batch(batch_size):
        # Extract the source texts
        inputs = [example[source_lang] for example in batch["translation"]]
        references_batch = [example[target_lang] for example in batch["translation"]]

        # Tokenize inputs
        inputs = tokenizer(inputs, return_tensors="tf", padding=True, truncation=True)

        # Generate translations
        outputs = model.generate(input_ids=inputs["input_ids"])
        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        # Append predictions and references
        predictions.extend(decoded_preds)
        references.extend([[ref] for ref in references_batch])

    # Compute the BLEU score using sacrebleu
    bleu = sacrebleu.corpus_bleu(predictions, references)
    return bleu.score

# Compute BLEU score on the validation dataset
bleu_score = compute_bleu_score(model, tokenizer, raw_datasets['validation'])
print(f"BLEU score: {bleu_score}")

input_text  = "How is everyone at home doing?"

tokenized = tokenizer([input_text], return_tensors='np')
out = model.generate(**tokenized, max_length=128)
print(out)
with tokenizer.as_target_tokenizer():
    print(tokenizer.decode(out[0], skip_special_tokens=True))