# -*- coding: utf-8 -*-
"""mt5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M8aWX6-anfiiQMKPODcNIFLzntO3tn7U
"""

pip install transformers datasets sacrebleu pandas

import pandas as pd
from datasets import Dataset

# Load your custom dataset
df = pd.read_csv("iitb.csv")
df.dropna(inplace=True)
# Convert the DataFrame to a Dataset object
dataset = Dataset.from_pandas(df)

# Split the dataset into train and validation sets
train_test_split = dataset.train_test_split(test_size=0.1)
train_data = train_test_split['train']
val_data = train_test_split['test']

from transformers import MT5Tokenizer, MT5ForConditionalGeneration

model_name = "google/mt5-small"

# Initialize the tokenizer
tokenizer = MT5Tokenizer.from_pretrained(model_name)

# Initialize the model
model = MT5ForConditionalGeneration.from_pretrained(model_name)

def preprocess_function(examples):
    inputs = examples["hindi"]
    targets = examples["english"]

    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length",return_tensors="pt")

    with tokenizer.as_target_tokenizer():
        labels = tokenizer([str(targets)for target in targets], max_length=128, truncation=True, padding="max_length",return_tensors="pt")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply the preprocessing function to the train and validation sets
train_data = train_data.map(preprocess_function, batched=True)
val_data = val_data.map(preprocess_function, batched=True)

from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)

import sacrebleu
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)
    bleu = sacrebleu.corpus_bleu(pred_str, [label_str])
    return {"bleu": bleu.score}

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./mt5-hi-en",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=2,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    logging_dir="./logs",
    logging_steps=10,
)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Ensure model weights are contiguous before saving
for name, param in model.named_parameters():
  if not param.is_contiguous():
    param.data = param.data.contiguous()

# Train the model
trainer.train()

# Evaluate the model
results = trainer.evaluate()
print(f"Evaluation results: {results}")

# Translate user input
def translate_sentence(sentence, model, tokenizer):
    inputs = tokenizer(sentence, return_tensors="pt", padding=True)
    outputs = model.generate(**inputs)
    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return translation

# Example usage:
user_input = "मुझे यह किताब पसंद है।"
translation = translate_sentence(user_input, model, tokenizer)
print(f"Input: {user_input}")
print(f"Translation: {translation}")